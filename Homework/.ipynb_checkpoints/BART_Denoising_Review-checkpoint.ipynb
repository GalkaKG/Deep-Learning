{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24cb0445-2cb9-4d03-82a4-def1b6d84c6c",
   "metadata": {},
   "source": [
    "# Article Review:  BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n",
    "\n",
    "### Authors:\n",
    "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov,Luke Zettlemoyer\n",
    "\n",
    "### Source:\n",
    "The article was presented at the __58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)__, a prestigious conference in the field of computational linguistics and natural language processing (NLP).\n",
    "\n",
    "Link to the article: https://arxiv.org/abs/1910.13461\n",
    "\n",
    "### Introduction:\n",
    "The article presents BART (Bidirectional and Auto-Regressive Transformers), a sequence-to-sequence model for natural language generation, translation, and comprehension. This approach combines ideas from both denoising autoencoders and autoregressive models, and it is aimed at improving the performance of various NLP tasks. The importance of this work lies in the application of denoising pre-training to improve sequence-to-sequence tasks, such as machine translation, summarization, and question answering.\n",
    "\n",
    "### Why is the topic important for deep learning?\n",
    "Deep learning techniques, particularly transformers and sequence-to-sequence models, have revolutionized the field of NLP. The article introduces an innovative method of pre-training using noise, which significantly enhances the model's ability to understand and generate human-like text. This is especially important in fields where large-scale text generation, translation, and comprehension are crucial, such as in conversational AI, content generation, and multilingual applications.\n",
    "\n",
    "### Main Objective of the Article:\n",
    "The main goal of the paper is to present and evaluate BART, a novel pre-trained model for a variety of NLP tasks. By leveraging denoising and autoregressive training strategies, BART achieves state-of-the-art performance on tasks such as text generation, translation, and comprehension. The authors demonstrate its effectiveness through empirical evaluations on benchmarks like SQuAD, CNN/Daily Mail, and XSum. The article aims to establish BART as an effective model for improving the efficiency and accuracy of sequence-to-sequence NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e0f02f-caf0-4abd-9f3c-7d40080c7214",
   "metadata": {},
   "source": [
    "### Architecture:\n",
    "\n",
    "BART adopts the standard transformer architecture but adjusts it for its unique objectives:\n",
    "- Encoder: Processes the corrupted input bidirectionally, capturing contextual information from the entire sequence.\n",
    "- Decoder: Autoregressively generates output, ensuring coherence and sequence fidelity.\n",
    "\n",
    "This design allows BART to perform well in both sequence-to-sequence tasks (e.g., summarization) and token-level tasks (e.g., classification).\n",
    "\n",
    "### Evaluation and Results:\n",
    "\n",
    "\n",
    "The authors tested BART across several NLP benchmarks and tasks, achieving state-of-the-art performance in many cases:\n",
    "\n",
    "- Text Summarization: Achieved leading results on CNN/DailyMail and XSum datasets.\n",
    "\n",
    "- Machine Translation: Demonstrated competitive performance without task-specific pretraining.\n",
    "\n",
    "- Question Answering: Performed well on the SQuAD dataset, showcasing its comprehension capabilities.\n",
    "\n",
    "- Text Generation: Produced high-quality outputs in generative tasks, rivaling GPT-like models.\n",
    "\n",
    "The experiments highlight BARTâ€™s versatility and efficacy, making it a powerful tool for diverse NLP applications.\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "BART's flexible architecture and robust pretraining make it suitable for a wide range of NLP tasks. It can be used for automatically generating concise summaries of lengthy texts, translating text between languages without the need for extensive fine-tuning, understanding context and generating accurate responses to queries, and enhancing chatbots and conversational AI systems.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "BART presents a novel approach to pre-training sequence-to-sequence models through denoising. This technique empowers BART to acquire robust language representations, leading to superior performance on various NLP tasks. The success of BART highlights the potential of denoising pre-training for advancing the field of natural language processing.\n",
    "\n",
    "### Code Availability\n",
    "\n",
    "The implementation of BART is accessible through the Hugging Face Transformers library, with detailed documentation and pre-trained models for fine-tuning:\n",
    "\n",
    "Official github repository: https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b76ebb-d7e0-4db3-ad15-df2fd07173c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01781010627746582,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "vocab.json",
       "rate": null,
       "total": 898823,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d36f425d83549f2835c6edca1bfc114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014588356018066406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "merges.txt",
       "rate": null,
       "total": 456318,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2cf6586ca549a2ba58df5876c58d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010057210922241211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "tokenizer.json",
       "rate": null,
       "total": 1355863,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef3ac75ea4e4117bbd19a33cce4862c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018460750579833984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "config.json",
       "rate": null,
       "total": 1585,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de71e90083f642958b28da3ae174f9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02168107032775879,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "model.safetensors",
       "rate": null,
       "total": 1625222120,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f89ca9572346d3bad3e4bafe4bf743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009698152542114258,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "generation_config.json",
       "rate": null,
       "total": 363,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed9806f6dcc4fa3b9b0713d97919842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cfacdc2-7473-4c96-ab01-66a6eb539c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart. In the end, itâ€˜s not the years in your life that count. Itâ€™s the life\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"\"\"The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart. In the end, itâ€˜s not the years in your life that count. Itâ€™s the life in your years.\"\"\"\n",
    "\n",
    "# Tokenization and generate\n",
    "inputs = tokenizer(text, max_length=1024, return_tensors=\"pt\", truncation=True)\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], max_length=50, min_length=20, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Summary text\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f4b91-5181-4743-b39a-85a678cbf033",
   "metadata": {},
   "source": [
    "# Results from BART Model Execution\n",
    "\n",
    "### **Original Text**\n",
    "> The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart. In the end, itâ€˜s not the years in your life that count. Itâ€™s the life in your years.\n",
    "\n",
    "### **Generated Summary**\n",
    "> The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart. In the end, itâ€˜s not the years in your life that count. Itâ€™s the life\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison Table**\n",
    "\n",
    "| Original Text                                                                                     | Generated Summary                                                                 |\n",
    "|---------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|\n",
    "| The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart. In the end, itâ€˜s not the years in your life that count. Itâ€™s the life in your years. | The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart. In the end, itâ€˜s not the years in your life that count. Itâ€™s the life |\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
