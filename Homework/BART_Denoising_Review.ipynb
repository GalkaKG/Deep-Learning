{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "666f88db-71b8-4ef7-8217-58808bb8add4",
   "metadata": {},
   "source": [
    "# Article Review: BART â€“ Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n",
    "\n",
    "## Authors\n",
    "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer\n",
    "\n",
    "## Source\n",
    "Presented at the **58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)**, a leading conference in computational linguistics and natural language processing (NLP).\n",
    "\n",
    "## Link to the Article\n",
    "ğŸ“„ [BART: Denoising Sequence-to-Sequence Pre-training](https://arxiv.org/abs/1910.13461)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Why This Topic Matters\n",
    "The article introduces **BART (Bidirectional and Auto-Regressive Transformers)**, a powerful sequence-to-sequence model designed for **natural language generation, translation, and comprehension**. It combines ideas from **denoising autoencoders** and **autoregressive models** to enhance performance across various NLP tasks.\n",
    "\n",
    "BART is particularly valuable for:\n",
    "- **Machine translation** ğŸš€\n",
    "- **Text summarization** ğŸ“„âœ‚ï¸\n",
    "- **Question answering** â“ğŸ”\n",
    "- **Conversational AI** ğŸ¤–ğŸ’¬\n",
    "\n",
    "By improving how models **understand and generate human-like text**, BART helps advance areas like **content generation, multilingual applications, and intelligent assistants**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Main Objective\n",
    "The primary goal of this paper is to introduce and evaluate **BART**, a **pre-trained model** that achieves state-of-the-art results across multiple NLP benchmarks. The authors show how **denoising and autoregressive pretraining** enhance model performance in sequence-to-sequence tasks.\n",
    "\n",
    "**Key Contributions:**\n",
    "- Proposes a **new pre-training approach** combining bidirectional encoding and autoregressive decoding.\n",
    "- Demonstrates **state-of-the-art** performance on **text summarization, translation, and comprehension** tasks.\n",
    "- Provides empirical results on benchmarks like **CNN/Daily Mail, SQuAD, and XSum**.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Architecture\n",
    "BART follows a **Transformer-based** architecture with key modifications:\n",
    "- **Encoder**: Processes **corrupted input** in a **bidirectional** manner, capturing deep contextual information.\n",
    "- **Decoder**: Generates output **autoregressively**, ensuring fluency and coherence.\n",
    "\n",
    "### ğŸ”„ Pre-Training Strategy\n",
    "BART is pre-trained using **denoising objectives**, where parts of the input text are **intentionally corrupted** and the model learns to reconstruct them. This improves the modelâ€™s ability to **handle missing, shuffled, or noisy text**.\n",
    "\n",
    "**Types of Noise Applied in Pretraining:**\n",
    "- **Token Deletion** â€“ Some words are randomly removed.\n",
    "- **Token Permutation** â€“ Word order is shuffled.\n",
    "- **Text Infilling** â€“ Random spans of text are replaced with placeholders.\n",
    "- **Sentence Shuffling** â€“ Sentence order is randomly altered.\n",
    "\n",
    "This approach enables BART to excel at tasks requiring **strong contextual understanding and text generation**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Evaluation and Results\n",
    "BART was tested across multiple NLP benchmarks, achieving impressive results:\n",
    "\n",
    "| Task | Dataset | Performance |\n",
    "|------|---------|-------------|\n",
    "| **Text Summarization** | CNN/DailyMail, XSum | **State-of-the-art** results ğŸ“ˆ |\n",
    "| **Machine Translation** | WMT English-German | Competitive results even **without fine-tuning** ğŸ”„ |\n",
    "| **Question Answering** | SQuAD | **Strong comprehension capabilities** âœ… |\n",
    "| **Text Generation** | Diverse datasets | **High-quality outputs**, rivaling GPT-like models ğŸ“ |\n",
    "\n",
    "### **Key Takeaways from Experiments**\n",
    "âœ” **BART outperforms previous models** on summarization and QA tasks.\n",
    "âœ” **It generalizes well across NLP tasks**, proving its robustness.\n",
    "âœ” **No task-specific modifications needed**, making it highly flexible.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒŸ Real-World Applications\n",
    "BARTâ€™s capabilities make it a versatile tool for:\n",
    "- **Automated summarization** ğŸ“° â†’ Condensing lengthy articles into key points.\n",
    "- **Machine translation** ğŸŒ â†’ Improving multilingual communication.\n",
    "- **Conversational AI** ğŸ¤– â†’ Powering chatbots and virtual assistants.\n",
    "- **Data augmentation** ğŸ“Š â†’ Generating synthetic training data for NLP tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ Conclusion\n",
    "BART introduces an **effective denoising-based pretraining technique** that enhances **natural language understanding and generation**. Its ability to perform well across diverse NLP tasks makes it a **foundational model** in modern AI research.\n",
    "\n",
    "âœ” **Novel pretraining strategy** improves sequence-to-sequence learning.  \n",
    "âœ” **Achieves state-of-the-art performance** across multiple NLP tasks.  \n",
    "âœ” **Highly flexible and easy to fine-tune** for different applications.  \n",
    "\n",
    "ğŸš€ **BART sets a new standard for NLP models, influencing the future of AI-driven text generation!**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‚ Code Availability\n",
    "The implementation of BART is available via the **Hugging Face Transformers** library:\n",
    "ğŸ”— **[Official GitHub Repository](https://github.com/huggingface/transformers)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b76ebb-d7e0-4db3-ad15-df2fd07173c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01781010627746582,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "vocab.json",
       "rate": null,
       "total": 898823,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d36f425d83549f2835c6edca1bfc114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014588356018066406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "merges.txt",
       "rate": null,
       "total": 456318,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2cf6586ca549a2ba58df5876c58d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010057210922241211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "tokenizer.json",
       "rate": null,
       "total": 1355863,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef3ac75ea4e4117bbd19a33cce4862c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018460750579833984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "config.json",
       "rate": null,
       "total": 1585,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de71e90083f642958b28da3ae174f9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02168107032775879,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "model.safetensors",
       "rate": null,
       "total": 1625222120,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f89ca9572346d3bad3e4bafe4bf743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009698152542114258,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "generation_config.json",
       "rate": null,
       "total": 363,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed9806f6dcc4fa3b9b0713d97919842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cfacdc2-7473-4c96-ab01-66a6eb539c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart. In the end, itâ€˜s not the years in your life that count. Itâ€™s the life\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"\"\"The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart. In the end, itâ€˜s not the years in your life that count. Itâ€™s the life in your years.\"\"\"\n",
    "\n",
    "# Tokenization and generate\n",
    "inputs = tokenizer(text, max_length=1024, return_tensors=\"pt\", truncation=True)\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], max_length=50, min_length=20, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Summary text\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f4b91-5181-4743-b39a-85a678cbf033",
   "metadata": {},
   "source": [
    "# Results from BART Model Execution\n",
    "\n",
    "### **Original Text**\n",
    "> The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart. In the end, itâ€˜s not the years in your life that count. Itâ€™s the life in your years.\n",
    "\n",
    "### **Generated Summary**\n",
    "> The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart. In the end, itâ€˜s not the years in your life that count. Itâ€™s the life\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison Table**\n",
    "\n",
    "| Original Text                                                                                     | Generated Summary                                                                 |\n",
    "|---------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|\n",
    "| The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart. In the end, itâ€˜s not the years in your life that count. Itâ€™s the life in your years. | The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart. In the end, itâ€˜s not the years in your life that count. Itâ€™s the life |\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
